---
title: "HW4"
authors: "Daniel Oliner, Musab Alquwaee, Jacob McGill"
output: md_document
date: "2024-04-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1
```{r, include=FALSE}
## put packages and data here
library(stats)
library(factoextra)
library(tidyverse)  
library(dplyr)
library(ggplot2)
library(readr)
library(cluster)
library(rsample)
library(mosaic)
library(reshape2)  
library(foreach)
library(caret)
library(igraph) 
library(arules)
library(arulesViz)
groceries = read.transactions("C:/Users/jacob/Downloads/groceries.txt", sep = ",")
wine = read.csv('C:/Users/jacob/Downloads/wine.csv', header = TRUE)
socialmarketing <- read.csv("C:/Users/jacob/Downloads/social_marketing.csv")
socialmarketing <- socialmarketing %>% select(-c(spam, adult, uncategorized, chatter))
non_numeric_cols <- socialmarketing %>% 
  summarise_all(~ any(!is.na(.) & !is.numeric(.))) %>%
  gather() %>%
  filter(value)


```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Since we are only interested in the chemical properties for PCA and clustering,we will exclude the last two columns (quality and color)
wine_chem = wine[, 1:11]

# Apply PCA
pca_results <- prcomp(wine_chem, scale. = TRUE, center = TRUE)

# Visualizing it
fviz_pca_ind(pca_results,
             col.ind = wine$color, # Color by wine color
             palette = c("#00AFBB", "#E7B800"),
             addEllipses = TRUE,
             legend.title = "Wine Color")

```

The PCA plot shows individuals represented in the space defined by the first two principal components. The color coding represents the type of wine, where one can clearly see a distinct grouping of red and white wines, indicating that the PCA is able to distinguish between red and white wines using the chemical properties alone.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Clustering with k-means 
set.seed(123) 
kmeans_results <- kmeans(scale(wine_chem), centers = 3, nstart = 25)

# Visualize clusters
fviz_cluster(kmeans_results, data = wine_chem,
             ellipse.type = "convex",
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
             ggtheme = theme_minimal())

```

The PCA biplot adds vectors for the chemical properties, showing their contribution to the two principal components. The separation between red and white wines appears significant along the first principal component, suggesting that some of these chemical properties are influential in differentiating wine color. However, there's no direct indication of wine quality in this plot.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# To color the points by actual wine color or quality
wine$color_numeric <- as.numeric(factor(wine$color))
# Plot PCA 
fviz_pca_biplot(pca_results,
                col.ind = wine$color_numeric, # Color by wine color
                col.var = "contrib", # Color variables by their contribution
                gradient.cols = c("#00AFBB", "#E7B800"),
                legend.title = "Wine Color")
```

The cluster plot reveals how the data points have been grouped based on the k-means clustering algorithm. The clusters seem to form distinct groups, but without a clear separation that would correspond to wine type (red vs. white). It's less clear than the PCA if the clusters align with wine color.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# we will also use fviz_cluster to color the points by quality score:
wine$quality_factor <- factor(wine$quality)

# Plotting with quality as a discrete factor
fviz_cluster(kmeans_results, data = wine_chem,
             ellipse.type = "convex",
             geom = "point", 
             ggtheme = theme_minimal()) +
  geom_point(aes(color = wine$quality_factor)) 
```

This plot shows a k-means clustering with nine clusters. It's a bit more difficult to interpret because the clusters are not clearly separated, and they don't seem to correlate with a single variable like wine color or quality. Instead, the clusters appear to be based on more subtle combinations of the chemical properties.

PCA outperforms other methods in distinguishing red from white wines due to clear separation and interpretable chemical property contributions. Quality distinction, however, is less evident, as it's subjectively rated and not solely dependent on chemical makeup. PCA's effectiveness in differentiating wine color suggests significant chemical profile variances between red and white wines. Yet, for nuanced aspects like quality, additional variables and advanced analysis techniques may be necessary.

# Question 2

## Introduction and Data Cleaning

This report analyzes social media interactions from followers of the NutrientH20 brand, using data collected over a seven-day period in June 2014. The goal is to identify distinct market segments within the social media audience to enhance targeted marketing strategies. By examining the categorized Twitter posts of these followers, this analysis seeks to uncover patterns and insights that could inform more effective and engaging marketing campaigns.

For simplicity, as part of the data cleaning process I have removed three variables from the dataset: 'Spam', 'Adult' and 'Uncategorized'. The rationale for this decision is that the "Spam", "Uncategorized", and "Chatter" variables will not help us in identifying and targeting specific market segments, which is the purpose of this endeavor. While, the "Adult" variable may be useful in identifying a specific market segment that could be targeted on certain NSFW websites, I have removed it in order to ensure a PG rating for this academic project. 

## Initial EDA

```{r, echo=FALSE}

category_summary <- socialmarketing %>%
  select(-X) %>%
  summarise_all(sum)
category_summary_long <- pivot_longer(category_summary, cols = everything(), names_to = "Category", values_to = "Mentions")

category_summary_long <- category_summary_long  %>%
  arrange(desc(Mentions))

ggplot(category_summary_long, aes(x = reorder(Category, -Mentions), y = Mentions)) +
  geom_bar(stat = "identity", fill = "dodgerblue") +
  theme_minimal() +
  labs(title = "Number of Mentions by Category", x = "Category", y = "Number of Mentions") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Beginning with a general preliminary data exploration, we see the number of mentions by category in Twitter posts by NutrientH20's followers over the surveyed period. We can see a high volume of posts related to photo sharing and health and nutrition, followed by cooking, politics, sports, etc. This visualisation indicates that NurtientH20's following tends to be interested in a health and wellness oriented, fitness-concsious, active lifestyle. In order to conduct a more detailed analysis and identify market segments within the brand's following, we will now utilize a correlation heatmap to visualize the relationship between categories, then use clustering to partition the brand's folllowing into segents based on similarities across individuals. 

## Market Segmentation: Correlation Heatmap
```{r, echo=FALSE, fig.width=7, fig.height=5, fig.keep="all", warning=FALSE}
correlations <- cor(socialmarketing %>% select(-X), use = "pairwise.complete.obs")

high_correlations <- which(abs(correlations) > 0.4 & upper.tri(correlations, diag = FALSE), arr.ind = TRUE)

# Extract indices of high correlations
high_correlation_indices <- data.frame(row = rownames(correlations)[high_correlations[, 1]],
                                       col = colnames(correlations)[high_correlations[, 2]])

# Create a new correlation matrix with only high correlations
high_correlation_matrix <- correlations[high_correlation_indices$row, high_correlation_indices$col]

heatmap <- ggplot(melt(high_correlation_matrix), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  theme_minimal() +
  labs(title = "High Correlation Heatmap", x = "Category", y = "Category", fill = "Correlation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels

print(heatmap)
```
We utilized a heatmap to visualize correlation between categories of interest as an initial step in the segmentation process. We see the strongest correlations between personal fitness & health and nutrition, beauty & fashion, beauty & cooking, religion & parenting, and religion & sports fandom (an interesting combination). Understanding these correlation gives us a clearer picture of which categories may be linked as shared interests, giving us a foundation for the next phase. We will now utilize clustering to group individuals into distinct segments based on the similarity of their interests, enabling us to craft tailored strategies for each unique cluster.

## Market Segmentation: Clustering
```{r, echo=FALSE, fig.width=7, fig.height=5, fig.keep="all", warning=FALSE}

socialmarketingx <- socialmarketing %>% select(-X)

k_grid <- seq(2, 10, by = 1) 

SSE_grid <- foreach(k = k_grid, .combine = 'c') %do% {
  kmeans_model <- kmeans(socialmarketingx, k, nstart = 50)
  kmeans_model$tot.withinss
}

plot(k_grid, SSE_grid, type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of Clusters (K)", ylab = "Total Within-Cluster Sum of Squares (WCSS)",
     main = "Elbow Method for Optimal K")

# Perform k-means clustering
set.seed(123)  # for reproducibility
k <- 3  # number of clusters, you can adjust this
kmeans_model <- kmeans(socialmarketingx, centers = 5)

socialmarketing_clustered <- cbind(socialmarketing, Cluster = kmeans_model$cluster)

# View the cluster centroids
cluster_centroids <- kmeans_model$centers

# Convert cluster centroids to a data frame
cluster_centroids_df <- as.data.frame(cluster_centroids)

# Add cluster labels to the cluster centroids data frame

cluster_centroids_df$Cluster <- factor(1:nrow(cluster_centroids_df)) 

cluster_centroids_long <- pivot_longer(cluster_centroids_df, cols = -Cluster, names_to = "Category", values_to = "Mean")

top_categories_per_cluster <- cluster_centroids_long %>%
  group_by(Cluster) %>%
  top_n(5, Mean) %>%  # Adjust this number to get more or fewer top categories
  arrange(Cluster, desc(Mean))

ggplot(top_categories_per_cluster, aes(x = reorder(Category, Mean), y = Mean, fill = Cluster)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  facet_wrap(~ Cluster, scales = "free_x") +
  theme_minimal() +
  labs(title = "Top Categories by Cluster", x = "Category", y = "Mean Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Market Segments and Potential Target Strategies

```{r, echo=FALSE, fig.width=9, fig.height=5, fig.keep="all", warning=FALSE}

cluster_names <- c("The Mixed Bag", "Fashionistas and Foodies", "Faith, Football, and Family", 
                   "Health and Fitness Enthusiasts", "College Gamers")
cluster_centroids_df$ClusterName <- cluster_names

top_categories_per_cluster <- inner_join(top_categories_per_cluster, cluster_centroids_df, by = "Cluster")

ggplot(top_categories_per_cluster, aes(x = reorder(Category, Mean), y = Mean, fill = Cluster)) +
  geom_bar(stat = "identity", position = position_dodge(width = 2)) +
  facet_wrap(~ ClusterName, scales = "free_x") +  
  theme_minimal() +
  labs(title = "Top Categories by Cluster", x = "Category", y = "Mean Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## College Gamers

The 'College Gamers' cluster exhibits a strong affinity towards gaming and collegiate life. They are tech-savvy, connected, and actively participate in online communities related to their interests.

### Marketing Strategies:

* **Partner With Streamers** Partner with popular Twitch streamers to facilitate product placement. 
* **Engage with eSports:** Sponsor eSports teams and tournaments to advertise at events.
* **Campus Events and Student Discounts:** Sponsor on-campus events to boost brand visibility.

## Health and Fitness Enthusiasts

Members of the 'Health and Fitness Enthusiasts' cluster are deeply invested in their personal fitness, healthy diet, and overall active lifestyle. 

### Marketing Strategies

* **Parnter with Fitness Influencers and Publications** Partner with fitness influencers and publications such as Men's Health to boost brand's reputation among health-conscious people.
* **Release a Healthier Version:** Develop and release a healthier version of the product, emphasizing its low-sugar content and non-use of artificial dyes and coloring. 
* **Sponsor Gyms, Yoga Studios, and Community Fitness Events ** Showcase the brand in gyms, yoga studios, and community fitness events (Longhorn Run!) to strengthen brand recognition and attract new customers who are passionate about fitness. 

## Faith, Football, and Family

The 'Faith, Football, and Family' cluster values community, shared experiences, and has a strong interest in sports and family-centric activities.

### Marketing Strategies

* **Youth Sports Sponsorships:** Sponsor local sports teams and tournaments, providing branded hydration stations at events. This highlights the beverage's benefit to athletes while emphasizing the brand's commitment to community values. 
* **Targeted Commercial Advertising:** Advertise during commercial breaks of major sports events, like Monday Night Football or March Madness. 
* **Charitable Partnerships:** Partner with religious organizations or charities to donate a portion of sales during specific holidays. Widely promote this partnership to further emphasize the brand's commitment to supporting local communities. 

## Fashionistas and Foodies

### Description

The 'Fashionistas and Foodies' cluster displayed a strong interest in the culinary arts, beauty, and fashion, indicating that they appreciate the finer things in life. Their significant interest in photo-sharing indicates a proclivity for visually documenting their culinary experiences or fasion choices. 

### Marketing Strategies

* **Influencer Partnerships:** Collaborate with fashion and food influencers to promote the brand by integrating the product into their lifestyle content narratives. 
* **Limited Edition 'Artist Series' Bottles:** Collaborate with artists to create a limited edition bottles that are visually stunning and collectible, then promote the bottle through notable influencers and celebrities in the fashion and culinary world. 
* **Paris Fashion Week: Hydration Partner** Align with Paris Fashion Week to become the Official Hydration Partner, featuring the 'Artist Series' bottles throughout the event.

## The Mixed Bag

The 'Mixed Bag' cluster presents a varied set of interests, suggesting a demographic that is not easily categorized but is open to a wide array of products and experiences. Versatile and wide-ranging marketing strategies will be needed for this cluster. 

### Marketing Strategies

* **Podcast Ad Reads:** Secure advertising partnerships with a variety of podcasts across genres including politics, current events, and travel.
* **Broad Social Media Ad Camaign:** Run a general ad campaign across multiple social media platforms, targeting a wide range of interests and demographics to maximize reach and brand visibility.

# Question 3

To mine for rules from the grocery transactions, I will first upload the txt file "Groceries" and read it as a transaction in order for arules to interact with it. To determine how to apply the a prior algorithm, I am going to look at a summary of the transactions.

```{r, echo = FALSE}
summary(groceries)
```
Based on the above results, I am going to limit the length of rules to a max length of 10. There are slightly below 10,000 transactions in the rules, so I am going to set support at a minimum value of 0.01 and confidence at 0.1.
```{r, include = FALSE}
groc_rules = apriori(groceries, 
                     parameter=list(support=.01, confidence=.1, maxlen=10))
```
```{r, include = FALSE}
inspect(groc_rules)
```
This produces about 435 rules, including 8 where the lhs is an empty subset. These rules are graphed against confidence and support, with color varying representing lift, are shown below

```{r, echo = FALSE, message = FALSE}
plot(groc_rules)
```

Looking at the graph, the rules with the strongest lift appear to be between 0 < support < 0.05 and 0 < confidence < 0.6. The lift ranges from 1 to just above 3. To hone in on more interesting/informative rules, I am going to limit the analysis to a subset of rules with confidence of at least 0.3 and a lift of at least 2.5, so that we are looking at the rules in the upper left part of the above graph. I chose this because the spread of rules is more sparse, so we may some more interesting connections. I also chose this lift to focus on rules that provide more useful rules, since lift measures the increases in probability of the lhs event given the rhs event.

```{r, echo = FALSE}
groc_sub = subset(groc_rules, subset=lift > 2 & confidence > 0.3)
inspect(groc_sub, arrange = ascending(lift))
```
This rule mining found some interesting relationships. For example, {citrus fruit, other vegetables} => {root vegetables} has a lift of 3.295 and a confidence of 0.359. Although root vegetables only occur with a 0.359 probability given a transaction contains citrus and other fruit, its probability increases by more than 3 times given that event occurs.  and {other vegetables, tropical fruit} => {root vegetables} has a lift of 3.144 and a confidence of about 0.343. These connections also make sense. For example, the rule {beef} => {root vegetables} has a relatively strong lift compared to the rest of the rules, coming in at 3.04. Considering that beef is often cooked with root vegetables like carrots (such as in stews or pot roasts). Furthermore, several rules show relationships between dairy products and fruit products (such as between curds and yogurt, or vegetables and butter). Finally, I am going to visualize the relationship between the goods in the transactions using Gephi.

```{r, include = FALSE}
groceries_graph = associations2igraph(subset(groc_rules, lift>2), associationsAsNodes = FALSE)
igraph::write_graph(groceries_graph, file='groceries.graphml', format = "graphml")
```

```{r, echo = FALSE}
knitr::include_graphics("C://Users/jacob/OneDrive/Documents/ECO395M-HW4/screenshot_final.png")
```

As can be seen, the most connected items in the transactions I filtered for are root vegetables, whole milk, and other vegetables. The most peripheral are berries and cream cheese, their only connection being with yogurt.

# Question 4

## Model Training

To train the classification model, I will set the batch size to 4 and train a CNN with 4 training epochs on the training set using PyTorch. I will measure the model performance by the finding the proportion of images it correctly classifies out of the entire set. The loss function will be CrossEntropyLoss.() and the optimizer function optim.Adam(net.parameters()). 
```{python, include = FALSE}
# Necessary Imports
import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import torch.optim as optim
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Set the directory where your data is stored
data_dir = 'C://Users/jacob/OneDrive/Documents/ECO395M-HW4/EuroSAT_RGB'
```

```{python, include = FALSE}
# Set the batch size for training and testing
batch_size = 4
# Define a transformation to apply to the images
transform = transforms.Compose(
    [transforms.Resize((32, 32)),  # Resize images to 32x32
     transforms.ToTensor(),  # Convert image to PyTorch Tensor data type
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # Normalize the images
```

```{python, include = FALSE}
# Load the training data
dataset = ImageFolder(root=data_dir, transform=transform)
#Create train test split
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
#Create data loaders for training and testing datasets
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
# Print some train samples to verify the data loading
data_iter = iter(train_loader)
images, labels = next(data_iter)
print(images.shape, labels.shape)
# Print some test samples to verify the data loading
data_iter = iter(test_loader)
images, labels = next(data_iter)
print(images.shape, labels.shape)
# Function to show an image
def imshow(img):
    img = img / 2 + 0.5  # Unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()
# Get some random training images
dataiter = iter(train_loader)
images, labels = next(dataiter)
# Show images
imshow(torchvision.utils.make_grid(images))
# Print labels
print(' '.join('%5s' % dataset.classes[labels[j]] for j in range(batch_size)))
```
```{python, include = FALSE, cache = TRUE}
# Define CNN to classify photos

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, 1)  # in_channels, out_channels, kernel_size, stride
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)  # Each channel gets zeroed out independently on each forward call with probability p
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(12544, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

net = Net()
print(net)
```
```{python, include = FALSE}
# Set the device to GPU if available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net.to(device)
# Define the loss function and the optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters())
# Number of training epochs
num_epochs = 5
trainiter = iter(train_loader)
```
```{python, include = FALSE, message = FALSE, cache = TRUE}
# Main training loop
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward, backward, and optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        if i % 5000 == 0:
            with torch.no_grad():
                try:
                    # Show a sample of images from the training data and model's predictions
                    train_images, train_labels = next(trainiter)
                except StopIteration:
                    trainiter = iter(test_loader)
                    train_images, test_labels = next(trainiter)
                train_images, train_labels = train_images.to(device), train_labels.to(device)

                imshow(torchvision.utils.make_grid(train_images[:8].cpu()))
                train_outputs = net(train_images[:8])
                _, predicted = torch.max(train_outputs, 1)
                print(' '.join('%5s' % dataset.classes[labels[j]] for j in range(batch_size)))

    # Print average loss for the epoch
    print(f"Epoch {epoch+1}, Loss: {running_loss / (i+1)}")


print('Finished Training')
```
## Model Testing

Now that the model is completed, we will test its accuracy on the test set. Accuracy in this case is simply the percentage of images it correctly identified from the test set.

```{python, include = FALSE}
# Set the model to evaluation mode
net.eval()

# Initialize variables to store total and correct predictions
total = 0
correct = 0

display_images = []
actual_labels = []
predicted_labels = []

# Iterate through the test data loader
for images, labels in test_loader:
    # Move data to the device
    images, labels = images.to(device), labels.to(device)
    
    # Forward pass
    outputs = net(images)
    
    # Get predicted labels
    _, predicted = torch.max(outputs, 1)
    
    # Update total count
    total += labels.size(0)
    
    # Update correct count
    correct += (predicted == labels).sum().item()

  # Update actual labels and predicted labels
    actual_labels.extend([dataset.classes[label] for label in labels])
    predicted_labels.extend([dataset.classes[label] for label in predicted])
    
  # Append a batch of images to the display_images list
    display_images.append(images.cpu())

# Compute accuracy
accuracy = correct / total

print(f'Accuracy on the test set: {accuracy:.2%}')
```
```{python, echo = FALSE}
# Compute accuracy
accuracy = correct / total

print(f'Accuracy on the test set: {accuracy:.2%}')
```

Finally, we will find the model's confusion matrix. The confusion matrix plots the number of images of certain class against how many of that class the model predicted. For example, it would list how many of the vegeation images from the test set the model classified as SeaLake. Predicted labels run along the x axis and actual labels along the y axis.

```{python, echo = FALSE}
# Convert labels to numerical values
label_to_index = {label: index for index, label in enumerate(dataset.classes)}
true_indices = [label_to_index[label] for label in actual_labels]
predicted_indices = [label_to_index[label] for label in predicted_labels]

# Compute the confusion matrix
conf_matrix = confusion_matrix(true_indices, predicted_indices)

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)
plt.xlabel('Predicted labels')
plt.ylabel('Actual labels')
plt.title('Confusion Matrix')
plt.show()
```

As can be seen above, the majority of incorrectly identified classes are in the double digits. However, there were 153 cases where Forest was predicted and the actual value was SeaLake.
